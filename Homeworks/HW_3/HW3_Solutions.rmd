---
title: "POL 150B/355B Homework 3 Solutions"
author: "Haemin Jee"
date: "Due: 2/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = T,
	warning = F
)
```

This homework assignment continues from the last, where we used *logistic regression* to perform supervised learning on text documents. In the current assignment, we'll be doing the same thing -- analyzing press releases for the presence of credit claiming -- only now with *LASSO*. We'll then compare the two models, LASSO and logistic regression, on  performance.

Run the code below (taken from the previous assignment) to get started.

```{r}
rm(list = ls())

# set wd
setwd("/Users/haeminjee/Dropbox/150B Machine Learning/Homeworks/HW_3")

# load libraries
library(glmnet)
library(ggplot2)

# Load `CreditClaim.RData` into R.
load("CreditClaim.RData")
dtm <- as.data.frame(credit_claim$x)
dtm$y <- credit_claim$y
```

## 1

Using a logistic regression, predict the credit claiming labels using all the words. What error message do you receive and what do you notice about the coefficients?

```{r}
#log_reg <- glm(y~., data = dtm, family = "binomial")
```
We get a ``algorithm did not converge" error message. There are many NAs for the coefficients. This is because we have more predictors than observations. 

## 2

Using the `glmnet` library, fit a LASSO regression. What do you notice about the total number of non-zero coefficients at different values of λ?

```{r}
words_matrix <- credit_claim$x
label_matrix <- as.matrix(dtm$y)

lasso <- glmnet(x = words_matrix, y = label_matrix, family = "binomial")
names(lasso)

gg <- as.data.frame(cbind(lasso$lambda, lasso$df))

ggplot(gg, aes(x=V1, y= V2)) + geom_line() + theme_bw() + xlab("Lambda") + ylab("Non-zero Coef") + ggtitle("Coefficients and Lambda")
```
As $\lambda$ increases, the total number of non-zero coefficients decrease. This make sense; as the penalty increases, more and more coefficients are being pushed to zero. 

## 3

What value of λ provides the highest accuracy? Print the non-zero coefficients for that model.

```{r}
# first make predictions using the lasso model
# this will return a 797x100 matrix  - predictions for each document for each value of lambda (100 lambdas by default)
lasso_predict <- predict(lasso, newx = words_matrix, type = "class")
class(lasso_predict) <- "numeric"

# writing a function to calculate accuracy
acc_func <- function(predicted, true){
  acc <- (sum(predicted & true) + sum(!predicted & !true)) / length(predicted)
  return(acc)
}

# using the apply function to apply the acc_func to columns of the prediction matrix
# acc_lasso will be a vector of 100 accuracy calculations
acc_lasso <- apply(lasso_predict, 2, FUN = acc_func, true = label_matrix)

# there are multiple lambda values that give the max accuracy so take the first one
lambda_max <- lasso$lambda[which(acc_lasso == max(acc_lasso))][1]
lambda_max

# creating a plot using ggplot
gg_df <- as.data.frame(cbind(acc_lasso, lasso$lambda))

g1 <- ggplot(gg_df, aes(x= V2, y = acc_lasso)) + geom_line() + theme_bw() + xlab("Lambdas") + ylab("Accuracy") + ggtitle("Accuracy and Lambda")
g1

# print coefficient of the model where lambda is set to the best lambda
coef_lasso <- coef(lasso, s = lambda_max)
# rounding the coefficients
coef_lasso <- round(coef_lasso[which(coef_lasso[,] !=0),] , 4)
sort(coef_lasso, decreasing = T)
length(coef_lasso)

```

## 4

The file `scores.csv` is a 4 x 3 spreadsheet containing the acccuracy, predicion, and recall scores for each of the 4 models we estimated in the last assignment. Load it into R.

```{r}
scores <- read.csv("scores.csv")
```

Compare the in sample accuracy of the linear probability models (10 and 20 word versions), the logistic regressions (10 and 20 word versions), and the LASSO model. Which model has the highest in sample accuracy?

```{r}
lasso_best_predict <- predict(lasso, newx = words_matrix, s = lambda_max, type = "class")

lasso_in_acc <- acc_func(as.numeric(lasso_best_predict), label_matrix) 

cat("In sample accuracy for LASSO model", "\n"); lasso_in_acc

cat("In sample accuracy for Linear model, 10 words", "\n"); scores[1,1]

cat("In sample accuracy for Linear model, 20 words", "\n"); scores[2,1]

cat("In sample accuracy for Logistic model, 10 words", "\n"); scores[3,1]

cat("In sample accuracy for Logistic, 20 words", "\n"); scores[4,1]
```
The LASSO model has the highest in-sample accuracy.

## 5

Perform leave one out cross validation for the LASSO model, providing the level of accuracy for each value of λ. How does the out of sample accuracy compare to the in sample accuracy?

[NB: This might take longer computing time.]

```{r}

# set seed for replication purposes
set.seed(pi)

# number of documents
ndocs <- dim(words_matrix)[1]

# empty matrix of predictions
loocv_predict <- matrix(NA, nrow = ndocs, ncol = length(lasso$lambda))

for(i in 1:ndocs){
  
  # create the training documents and labels, taking out the ith document / label
  train_docs <- words_matrix[-i,]
  train_labels <- label_matrix[-i]
  
  # run the lasso model with the same lambdas as previous lasso model
  lasso_mod <- glmnet(x=train_docs, y=train_labels, family = "binomial", lambda = lasso$lambda)
  
  # make predictions on the ith document, using the model developed by the training data
  loocv_predict[i,] <- predict(lasso_mod, newx = t(words_matrix[i,]), type = "class")
}

# compute accuracy for each lambda
class(loocv_predict) <- "numeric"
acc_lasso_cv <- apply(loocv_predict, 2, FUN = acc_func, true = label_matrix)

plot(x=lasso$lambda, y=acc_lasso, type = "l", main = "In-Sample and Out-Sample Accuracy", 
     xlab = "Lambda", ylab =  "Accuracy")
lines(lasso$lambda, y=acc_lasso_cv, col = "red")
legend("topright", c("In-Sample", "Out-sample"), col = c("black", "red"), lwd = c(2.5, 2.5), cex = 0.6)

```
The plot above shows the in-sample and out-of-sample accuracy as a function of $\lambda$. Increasing $\lambda$ leads to modest increases out-of-sample accuracy, but these increases quickly taper off. This intiial increase is because increasing $\lambda$ shrinks many of the coefficients to reduce over-fitting. 

```{r}
# comparing in-sample vs. out-sample accuracy
max(acc_lasso_cv)
```
The max accuracy for the out-of-sample accuracy is .84, significantly less than 1 (the max accuracy for in-sample). 