---
title: "POL 150B/350B Homework 2"
date: "Due: 1/30/2018"
author: Tongtong Zhang
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = T,
	warning = F
)
```

In this homework assignment we will use linear and logistic regression to perform *supervised learning* on text documents.

Supervised learning methods are methods for classifying observations into a pre-determined set of categories. When applied to text, the goal of supervised learning is to classify a collection of documents into categories. For example, we might want to know if tweets are positive or negative, if a speech is liberal or conservative, or if a conversation is about politicis or not. We could perform the analysis by hand, but that is often cumbersome and time consuming. Supervised learning methods subsidize hand coding and reduce the cost of analyzing text documents.

Supervised learning methods begin just like traditional hand coding classification. Researchers have a set of predetermined categories and then they (or research assistants) place a randomly selected sample of documents into those categories. This constitutes a *training set* that we will use to train an algorithm for classification. That is, the hand coding decisions will be our *dependent variable*. We will also refer to the hand classification decision as the *labels* for the documents.

To use the hand coded documents for classification we need to convert the texts to data. To do this, we represent the texts as a count vector. We will describe this process later in the course (you can get a preview if you want, the slides posted for the 5th lecture describe the process), but the basic intuition is that we count the number of times a set of words occurs across our documents and then use the count of words in each document as a predictor. In other words, the count of each word in each document will constitute our *predictors*.

Using the labels and the word counts, we then use a prediction algorithm to learn the relationship between word counts and labels. For example, you might use logistic regression to regress the hand coded decision on the word counts. We can then use this model to formulate predictions for the unlabeled documents.

The result of the process is that all documents are classified into categories. The benefit is that the automatic classification is much faster than the hand classification.

We can write out supervised learning in the following steps:

1. Hand classify randomly sampled documents into a set of predetermined categories. This is the training set and we will call the classification of each document its label.

2. Represent the documents in the training set as count vectors. Each entry of the count vector will count the number of times that particular word occurred in that document.

3. With the hand-coded documents and the count vectors use a classification algorithm to learn the relationship between the predictors and the labels.

4. Finally, the relationship is used to classify the unlabeled documents.

# Credit Claiming in Congressional Texts

In *The Impression of Influence*, Grimmer, Westwood, and Messing analyze the rate members of Congress claim credit for government spending in their press releases. Members of Congress issue a lot of press releases: from 2005 to 2010, House members issues nearly *170,000* press releases.

Given that it would be hard to analyze such a large collection by hand, GWM decided to use supervised learning methods. They hired a team of Stanford undergraduates to classify a random sample of 800 press releases as credit claiming or not. They then represented the texts as a count vector.

The object `CreditClaim.RData` contains the list `credit_claim`. The first element of this list is the count vector representation of the documents (labeled `x`, sometimes called a *document term matrix*) and the second element are the labels (`y`).

## 1

To get started, load `CreditClaim.RData` into R. Create an object `dtm` that holds the document term matrix. Create an object `y` that contains the labels.

```{r}
rm(list=ls())
setwd("/Users/tongtongzhang/Desktop/Stanford/Year 4/TA Machine Learning/Homeworks-for-TAs/HW_2")
load("CreditClaim.RData")

dtm<-credit_claim[[1]] #document-term matrix
y<-credit_claim[[2]] #labels
```

## 2

Identify the twenty words that are the most prevalent (occur most often) across the documents. Print those twenty words and comment briefly: what do you notice about the words?

**Hint** `colSums` and `sort` are your friends.

```{r}
word_count<-colSums(dtm) #calculate the number of each word across documents
names(word_count)<-colnames(dtm)

sort(word_count,decreasing = T)[1:20] #Get the top 20 words that show up most across documents and print them out
```

The most frequent word "congress" is not surprising given these are all press releases from members of the Congress. The high frequency of words like "funding", "tax", and "million" show that members of Congress frequently talked about money and spending in their press releases. The other words in the top 20 signal considerable credit-claiming in press releases, including words like "care", "support", "help", and "people".

## 3

Create 2 objects: `dtm.20` should contain all the documents with only the 20 most common words. `dtm.10` should contain only the 10 most common words.

Then add a column to both dataframes called `y` that holds the labels for those documents.

```{r}
dtm.20 <- data.frame(dtm[,names(sort(word_count,decreasing = T)[1:20])]) #subset dtm to only the columns with the top 20 words
dtm.20$y<-y #add "y" as a column to the dataframe

dtm.10 <- data.frame(dtm[,names(sort(word_count,decreasing = T)[1:10])]) #subset dtm to only the columns with the top 10 words
dtm.10$y<-y #add "y" as a column to the dataframe
```

## 4

We are going to predict the label using a subset of the document term matrix in four separate models. Print a summary for all four models.

a) Predict the credit claiming label with the 10 most common words, using a linear probability model. Call this `model_1`
b) Predict the credit claiming label with the 10 most common words, using a logistic regression. Call this `model_2`
c) Predict the credit claiming label with the 20 most common words, using a linear probability model. Call this `model_3`
d) Predict the credit claiming label with the 20 most common words, using a logistic regression. Call this `model_4`

```{r}
# Model 1: top 10 words, linear model
model1<-lm(y ~., data=dtm.10)
summary(model1) #"dateline" dropped as it has perfect collinearity with "byline"
model_1<-predict.lm(model1) #predicted prob. of y=1

# Model 2: top 10 words, logistic model
model2 <- glm(y ~.,family=binomial(link='logit'),data=dtm.10)
summary(model2)
model_2<-predict.glm(model2,type="response") #predicted prob. of y=1

# Model 3: top 20 words, linear model
model3<-lm(y ~., data=dtm.20)
summary(model3) #"dateline" dropped as it has perfect collinearity with "byline"
model_3<-predict.lm(model3)

# Model 4: top 20 words, logistic model
model4 <- glm(y ~.,family=binomial(link='logit'),data=dtm.20)
summary(model4)
model_4<-predict.glm(model4,type="response") 
```

## 5

Create 3 plots:
a) The preditions of model 1 against the predictions for model 2.
b) The preditions of model 1 against the predictions for model 3.
c) The preditions of model 3 against the predictions for model 4.

```{r}
# a) model_1 vs. model_2
plot(model_1 ~ model_2, xlab="predictions of model 2", ylab="predictions of model 1", pch=20)
abline(0,1,col="red") #superimpose a 45 degree line

# b) model_1 vs. model_3
plot(model_1 ~ model_3, xlab="predictions of model 3", ylab="predictions of model 1", pch=20)
abline(0,1,col="red") #superimpose a 45 degree line

# c) model_3 vs. model_4
plot(model_3 ~ model_4, xlab="predictions of model 4", ylab="predictions of model 3", pch=20)
abline(0,1,col="red") #superimpose a 45 degree line
```

## 6

Using a threshold of 0.5, classify each document as credit claiming or not. Compare the classifications across the models.
```{r}
# Classify documents into 1 or 0 for each model
model_1_class<-ifelse(model_1 >= 0.5, 1, 0)
model_2_class<-ifelse(model_2 >= 0.5, 1, 0)
model_3_class<-ifelse(model_3 >= 0.5, 1, 0)
model_4_class<-ifelse(model_4 >= 0.5, 1, 0)

# Compare classifications btw each two models (cross-tab)
table(model_1_class, model_2_class)
table(model_1_class, model_3_class)
table(model_1_class, model_4_class)
table(model_2_class, model_3_class)
table(model_2_class, model_4_class)
table(model_3_class, model_4_class)
```

## 7

Provide the accuracy, precision, and recall for each model. Compare the in-sample performance across models.

```{r}
# Confusion matrix for each model
conf_1<-table(model_1_class, y)
conf_2<-table(model_2_class, y)
conf_3<-table(model_3_class, y)
conf_4<-table(model_4_class, y)

# Accuracy: model 4 (20 words + logistic) performs best
(conf_1[1,1] + conf_1[2,2])/nrow(dtm) #model 1
(conf_2[1,1] + conf_2[2,2])/nrow(dtm) #model 2
(conf_3[1,1] + conf_3[2,2])/nrow(dtm) #model 3
(conf_4[1,1] + conf_4[2,2])/nrow(dtm) #model 4

# Precision: model 3 (20 words + linear) performs best
conf_1[2,2]/sum(conf_1[2,])#model 1
conf_2[2,2]/sum(conf_2[2,])#model 2
conf_3[2,2]/sum(conf_3[2,]) #model 3
conf_4[2,2]/sum(conf_4[2,]) #model 4

# Recall: model 4 (20 words + logistic) performs best
conf_1[2,2]/sum(conf_1[,2]) #model1
conf_2[2,2]/sum(conf_2[,2]) #model2
conf_3[2,2]/sum(conf_3[,2]) #model3
conf_4[2,2]/sum(conf_4[,2]) #model4
```

Overall, it looks like Model 4 (20 most frequent words + logistic model) performs best in the in-sample prediction. It has the highest accuracy and recall across models.

## 8

Now we are going to compare the in-sample fit to the out-of-sample fit. To do this, weâ€™re going to use leave one out cross validation (LOOCV).

For each of document, perform the following procedure for both the LPM model with 20 predictors and the logistic regression with 20 features:
  - For document *i* Fit the LPM and logistic regression to all documents but *i* (we leave this document out of the model)
  - Make a prediction for document *i* using both the LPM and logistic regression and a classification for the document, with a 0.5 threshold

```{r}
######################## Method 1: Using For-loops ########################
#empty vectors to store if the out- of-sample prediction matches with the actual label (1) or not (0)
model3_LOOCV_accurate<-rep(NA,nrow(dtm))
model4_LOOCV_accurate<-rep(NA,nrow(dtm))

# Loop over each document
for(i in 1:nrow(dtm)){
  model3_temp<-lm(y~., data=dtm.20[-i,]) # fit model 3 leaving out 1 obs.
  model3_pred<-predict.lm(model3_temp, newdata=dtm.20[i,1:20]) # predict that ommitted obs
  model3_class<- as.numeric(model3_pred >= 0.5) #classify the prediction using 0.5 rule
  model3_LOOCV_accurate[i]<-model3_class == dtm.20$y[i] #store if the predicted label is the same as the actual label
  
  # Same steps for model 4
  model4_temp<-glm(y ~.,family=binomial(link='logit'),data=dtm.20[-i,])
  model4_pred<-predict.glm(model4_temp,type="response",newdata=dtm.20[i,1:20])
  model4_class<- as.numeric(model4_pred >= 0.5)
  model4_LOOCV_accurate[i]<-model4_class == dtm.20$y[i]
}


mean(model3_LOOCV_accurate)
mean(model4_LOOCV_accurate)


########### Method 2: Using canned functions "cv.glm" in R ############
require(boot)
cost <- function(y, pi) mean(abs(y-pi) > 0.5) #define the loss function in cross-validation: if abs(y-pi) <= 0.5 (right prediction), will return 0; if abs(y-pi) > 0.5 (wrong prediction), will return 1

model3<-glm(y~.,family=gaussian,data=dtm.20)
1-cv.glm(dtm.20,model3,cost=cost)$delta[1] #accuracy for model 3
1-cv.glm(dtm.20,model4,cost=cost)$delta[1] #accuracy for model 4
```

## 9

How does the accuracy from the in sample fit (Question 7) compare to the accuracy for the out of sample fit (Question 8)?

```{r}
# Model 3: In-sample fit has higher accuracy
(conf_3[1,1] + conf_3[2,2])/nrow(dtm) #in-sample accuracy
mean(model3_LOOCV_accurate) #out of sample accuracy

# Model 4: In-sample fit has higher accuracy
(conf_4[1,1] + conf_4[2,2])/nrow(dtm) #in-sample accuracy
mean(model4_LOOCV_accurate) #out of sample accuracy
```

In both models, the in-sample fit still has higher accuracy than the out-of-sample fit.